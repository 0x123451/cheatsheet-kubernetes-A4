* Kubernets CheatSheet                                             :Kubernetes:
:PROPERTIES:
:type:     kubernetes
:END:

Blog URL: https://cheatsheet.dennyzhang.com/cheatsheet-kubernetes, Category: [[https://cheatsheet.dennyzhang.com/category/kubernetes/][kubernetes]]

#+BEGIN_HTML
<a href="https://github.com/dennyzhang/cheatsheet-kubernetes-A4"><img align="right" width="200" height="183" src="https://www.dennyzhang.com/wp-content/uploads/denny/watermark/github.png" /></a>
<div id="the whole thing" style="overflow: hidden;">
<div style="float: left; padding: 5px"> <a href="https://www.linkedin.com/in/dennyzhang001"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a></div>
<div style="float: left; padding: 5px"><a href="https://github.com/dennyzhang"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a></div>
<div style="float: left; padding: 5px"><a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img src="https://slack.dennyzhang.com/badge.svg" alt="slack"/></a></div>
</div>

<br/><br/>
<a href="http://makeapullrequest.com" target="_blank" rel="nofollow"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"/></a>
#+END_HTML

File me [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/issues][Issues]] or star [[https://github.com/DennyZhang/cheatsheet-kubernetes-A4][this repo]].

See more CheatSheets from Denny: [[https://github.com/topics/denny-cheatsheets][#denny-cheatsheets]]
** Common Usage
*** yaml templates
| Name                         | Summary                                                                        |
|------------------------------+--------------------------------------------------------------------------------|
| Pod yaml examples            | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-dummy.yaml][pod-dummy.yaml]], [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-nginx.yaml][pod-nginx.yaml]],  [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-volume-redis.yaml][pod-volume-redis.yaml]]                         |
| Pod yaml examples            | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-healthcheck-nginx.yaml][pod-healthcheck-nginx.yaml]], [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-gitclone.yaml][pod-gitclone.yaml]], [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-handlers.yaml][pod-handlers.yaml]]               |
| Deployment yaml examples     | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/deployment-nginx.yaml][deployment-nginx.yaml]]                                                          |
| Service yaml examples        | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/service-clusterip-nginx.yaml][service-clusterip-nginx.yaml]]                                                   |
| Volume yaml examples         | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/pod-volume-empty-redis.yaml][pod-volume-empty-redis.yaml]]                                                    |
| Statefulset yaml examples    | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/statefulset-nginx.yaml][statefulset-nginx.yaml]], [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/statefulset-single-mysql][statefulset-single-mysql]], [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/statefulset-replicated-mysql][statefulset-replicated-mysql]] |
| Serviceaccount yaml examples | [[https://github.com/dennyzhang/cheatsheet-kubernetes-A4/blob/master/serviceaccount-default.yaml][serviceaccount-default.yaml]]                                                    |

*** Common Commands
| Name                                   | Command                                                               |
|----------------------------------------+-----------------------------------------------------------------------|
| Validate yaml file with dry run        | =kubectl create --dry-run --validate -f pod-dummy.yaml=               |
| Start a temporary pod to run wget test | =kubectl run --rm wget-test --image=busybox --restart=Never --tty -i= |
| Start a temporary pod to run curl test | =kubectl run --rm curl-test --image=yauritux/busybox-curl --tty -i=   |
| Open a bash terminal in a pod          | =kubectl exec -it storage sh=                                         |
| Check pod environment variables        | =kubectl exec redis-master-ft9ex env=                                 |
| Get all services for all namespace     | =kubectl get service --all-namespaces=                                |
| Get system conf via configmap          | =kubectl -n kube-system get cm kubeadm-config -o yaml=                |
| Query healthcheck endpoint             | =curl -L http://127.0.0.1:10250/healthz=                              |

*** Check Performance
| Name                                         | Command                                              |
|----------------------------------------------+------------------------------------------------------|
| Get node resource usage                      | =kubectl top node=                                   |
| Get node resource usage                      | =kubectl top pod=                                    |
| List resource utilization for all containers | =kubectl top pod --all-namespaces --containers=true= |
** Pod
| Name                         | Command                                                                                                                                 |
|------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------|
| List all pods                | =kubectl get pods=                                                                                                                      |
| List pods for all namespace  | =kubectl get pods -all-namespaces=                                                                                                      |
| List all critical pods       | =kubectl get -n kube-system pods -a=                                                                                                    |
| List pods with more info     | =kubectl get pod -o wide=, =kubectl get pod -o yaml=                                                                                    |
| Get pod info                 | =kubectl describe pod srv-mysql-server=                                                                                                 |
| List all pods with labels    | =kubectl get pods --show-labels=                                                                                                        |
| Get Pod initContainer status | kubectl get pod dummy --template '{{.status.initContainerStatuses}}'                                                                    |
| Get pod by selector          | podname=$(kubectl get pods -n $namespace --selector="app=syslog" -o jsonpath='{.items[*].metadata.name}')                               |
| List pods with docker images | kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}:{.spec.containers[0].name}{"\t"}{.spec.containers[0].image}{"\n"}{end}' |
| kubectl run command          | kubectl exec -it -n "$namespace" "$podname" -- sh -c "echo $msg  >>/dev/termination-log"                                                |

- Delete Pod
| Name                | Command                                     |
|---------------------+---------------------------------------------|
| Delete pod          | =kubectl delete pod hello-node-95913-n63qs= |
| Delete pod by label | =kubectl delete pod -l env=test=            |

** Label & Annontation
| Name                             | Command                                                               |
|----------------------------------+-----------------------------------------------------------------------|
| Filter pods by label             | =kubectl get pods -l owner=denny=                                     |
| Manually add label to a pod      | =kubectl label pods dummy-input owner=denny=                          |
| Remove label                     | =kubectl label pods dummy-input owner-=                               |
| Manually add annonation to a pod | =kubectl annotate pods dummy-input my-url=https://www.dennyzhang.com= |

** Deployment & Scale
[[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment][link: Pausing and Resuming a Deployment]]

| Name                         | Command                                                       |
|------------------------------+---------------------------------------------------------------|
| Scale out                    | =kubectl scale --replicas=3 deployment/nginx-app=             |
| online rolling upgrade       | =kubectl rollout app-v1 app-v2 --image=img:v2=                |
| Roll backup                  | =kubectl rollout app-v1 app-v2 --rollback=                    |
| List rollout                 | =kubectl get rs=                                              |
| Check update status          | =kubectl rollout status deployment/nginx-app=                 |
| Check update history         | =kubectl rollout history deployment/nginx-app=                |
| Pause/Resume                 | =kubectl rollout pause deployment/nginx-deployment=, =resume= |
| Rollback to previous version | =kubectl rollout undo deployment/nginx-deployment=            |

** Service
| Name                     | Command                                                                             |
|--------------------------+-------------------------------------------------------------------------------------|
| List all services        | =kubectl get services=                                                              |
| Get service detail       | =kubectl get service nginx-service -o yaml=                                         |
| Get service cluster ip   | kubectl get service nginx-service -o go-template='{{.spec.clusterIP}}'              |
| Get service cluster port | kubectl get service nginx-service -o go-template='{{(index .spec.ports 0).port}}'   |
** StatefulSet
| Name                               | Command                                                                        |
|------------------------------------+--------------------------------------------------------------------------------|
| List statefulset                   | =kubectl get sts=                                                              |
| Scale statefulset                  | =kubectl scale sts <stateful_set_name> --replicas=5=                           |
| Delete statefulset only (not pods) | =kubectl delete sts <stateful_set_name> --cascade=false=                       |

** Volumes & Volume Claims
| Name                      | Command                         |
|---------------------------+---------------------------------|
| Check the mounted volumes | =kubectl exec storage ls /data= |
| Check persist volume      | =kubectl describe pv pv0001=    |
** Other Components
*** Log files
| Name                           | Command                                 |
|--------------------------------+-----------------------------------------|
| API Server.log= in master node | =/var.log=/kube-apiserver.log=          |
| Scheduler.log= in master node  | =/var.log=/kube-scheduler.log=          |
| Controller.log= in master node | =/var.log=/kube-controller-manager.log= |
| Kubelet.log= in worker node    | =/var.log=/kubelet.log=                 |
| Kube Proxy.log= in worker node | =/var.log=/kubelet-proxy.log=           |

*** Events & Metrics
| Name            | Command                               |
|-----------------+---------------------------------------|
| View all events | =kubectl get events --all-namespaces= |

*** Namespace & Security
| Name                          | Command                                     |
|-------------------------------+---------------------------------------------|
| List authenticated contexts   | =kubectl config get-contexts=               |
| List contexts                 | =kubectl config get-contexts=               |
| Switch context                | =kubectl config use-context <cluster-name>= |
| List all namespaces defined   | =kubectl get namespaces=                    |
| kubectl config file           | =~/.kube/config=                            |

*** Network
| Name                              | Command                                 |
|-----------------------------------+-----------------------------------------|
| Temporarily add a port-forwarding | =kubectl port-forward redis-izl09 6379= |

*** Endpoint
| Name           | Command                 |
|----------------+-------------------------|
| List endpoints | =kubectl get endpoints= |

** Basic
*** Key Concepts
| Name | Summary                           |
|------+-----------------------------------|
| CNCF | Cloud Native Computing Foundation |
| CRI  | Container Runtime Interface       |
| CNI  | Container Network Interface       |
| CSI  | Container Storage Interface       |

*** Kubernets Critical Files
| Name                      | Comment                                                 |
|---------------------------+---------------------------------------------------------|
| Config folder             | =/etc/kubernetes/=                                      |
| Certificate files         | =/etc/kubernetes/pki/=                                  |
| Credentials to API server | =/etc/kubernetes/kubelet.conf=                          |
| Superuser credentials     | =/etc/kubernetes/admin.conf=                            |
| Kubernets working dir     | =/var/lib/kubelet/=                                     |
| Docker working dir        | =/var/lib/docker/=                                      |
| Etcd working dir          | =/var/lib/etcd/=                                        |
| Network cni               | =/etc/cni/net.d/=                                       |
| Docker container log      | =/var/log/containers/=                                  |
| Log files                 | =/var/log/pods/=                                        |
| Env                       | =export KUBECONFIG=/etc/kubernetes/admin.conf=          |
| Env                       | =/etc/systemd/system/kubelet.service.d/10-kubeadm.conf= |

 #+BEGIN_HTML
 <a href="https://www.dennyzhang.com"><img align="right" width="185" height="37" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/dns_small.png"></a>
 #+END_HTML
*** Check status
| Name                               | Summary                                      |
|------------------------------------+----------------------------------------------|
| Start a service                    | =kubectl run hello --image=my_img --port=80= |
| Similar to =docker ps=             | =kubectl get nodes=                          |
| Similar to =docker inspect=        | =kubectl describe pod nginx-app-413181-cn=   |
| Similar to =docker logs=           | =kubectl logs=                               |
| Similar to =docker exec=           | =kubectl exec=                               |
| Get deployment info                | =kubectl get deploy=                         |
| Delete service                     | =kubectl delete service nginxservice=        |
| Get kubectl version                | =kubectl version=                            |
| Get cluster info                   | =kubectl cluster-info=                       |
| Get configuration                  | =kubectl config view=                        |
| Get component status               | =kubectl get componentstatus=                |
| Get node status                    | =kubectl describe node $node_name=           |
| Get services for current namespace | =kubectl get svc=                            |

*** Kubernetes Developer Resources
| Name            | Summary                                                                                   |
|-----------------+-------------------------------------------------------------------------------------------|
| API Conventions | https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md |
** Minikube
[[https://github.com/kubernetes/minikube][link: minikube in GitHub]]
| Name                | Command                       |
|---------------------+-------------------------------|
| Start minikube env  | =minikube start=              |
| minikube docker-env | =eval $(minikube docker-env)= |
| Get dashboard       | =minikube dashboard=          |
| ssh to minikube vm  | =minikube ssh=                |
| Get ip              | =minikube ip=                 |
| Get cluster info    | =kubectl cluster-info=        |
| List addons         | =minikube addons list=        |
| Get service info    | =minikube service $srv_name=  |

TODO: rolling-update command is imperative, better use Deployments rollout. It's declarative.
** Misc scripts
- Tail pod log by label
#+BEGIN_SRC sh
namespace="mynamespace"
mylabel="app=mylabel"
kubectl get pod -l "$mylabel" -n "$namespace" | tail -n1 \
    | awk -F' ' '{print $1}' | xargs -I{} \
      kubectl logs -n "$namespace" -f {}
#+END_SRC

- Get node hardware resource utilization
#+BEGIN_SRC sh
kubectl get nodes --no-headers \
     | awk '{print $1}' | xargs -I {} \
     sh -c 'echo {}; kubectl describe node {} | grep Allocated -A 5'

kubectl get nodes --no-headers | awk '{print $1}' | xargs -I {} \
    sh -c 'echo {}; kubectl describe node {} | grep Allocated -A 5 | grep -ve Event -ve Allocated -ve percent -ve -- ; echo'
#+END_SRC

- Apply the configuration in manifest.yaml and delete all the other configmaps that are not in the file.

#+BEGIN_EXAMPLE
kaubectl apply --prune -f manifest.yaml --all --prune-whitelist=core/v1/ConfigMap
#+END_EXAMPLE
** More Resources
 License: Code is licensed under [[https://www.dennyzhang.com/wp-content/mit_license.txt][MIT License]].

 - Useful links
 #+BEGIN_EXAMPLE
 - https://kubernetes.io/docs/reference/kubectl/cheatsheet/
 - https://github.com/kubecamp/kubernetes_in_2_days
 - https://marc.xn--wckerlin-0za.ch/computer/kubernetes-on-ubuntu-16-04
 - https://codefresh.io/kubernetes-guides/kubernetes-cheat-sheet/
 #+END_EXAMPLE

 #+BEGIN_HTML
 <a href="https://www.dennyzhang.com"><img align="right" width="201" height="268" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/denny_201706.png"></a>

 <a href="https://www.dennyzhang.com"><img align="right" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/dns_small.png"></a>
 #+END_HTML
* org-mode configuration                                           :noexport:
#+STARTUP: overview customtime noalign logdone showall
#+DESCRIPTION: 
#+KEYWORDS: 
#+AUTHOR: Denny Zhang
#+EMAIL:  denny@dennyzhang.com
#+TAGS: noexport(n)
#+PRIORITIES: A D C
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: exclude noexport
#+SEQ_TODO: TODO HALF ASSIGN | DONE BYPASS DELEGATE CANCELED DEFERRED
#+LINK_UP:   
#+LINK_HOME: 
* #  --8<-------------------------- separator ------------------------>8-- :noexport:
* [#A] Kubernets                                         :noexport:IMPORTANT:
https://github.com/dennyzhang/cheatsheet-kubernetes-A4

k8s provides declarative primitives for the "desired state"
- Self-healing
- Horizontal scaling
- Automatic binpacking
- Service discovery and load balancing
** Names of certificates files
https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md
Names of certificates files:
ca.crt, ca.key (CA certificate)
apiserver.crt, apiserver.key (API server certificate)
apiserver-kubelet-client.crt, apiserver-kubelet-client.key (client certificate for the apiservers to connect to the kubelets securely)
sa.pub, sa.key (a private key for signing ServiceAccount )
front-proxy-ca.crt, front-proxy-ca.key (CA for the front proxy)
front-proxy-client.crt, front-proxy-client.key (client cert for the front proxy client)
** DONE [#A] k8s yaml file
  CLOSED: [2017-12-01 Fri 22:45]
http://containertutorials.com/get_started_kubernetes/k8s_example.html
https://www.mirantis.com/blog/introduction-to-yaml-creating-a-kubernetes-deployment/
https://kubernetes.io/docs/concepts/storage/volumes/#emptydir
** TODO update k8s cheatsheet github: https://github.com/alex1x/kubernetes-cheatsheet
** TODO Setting up MySQL Replication Clusters in Kubernetes: https://blog.kublr.com/setting-up-mysql-replication-clusters-in-kubernetes-ab7cbac113a5
** TODO MySQL on Docker: Running Galera Cluster on Kubernetes
https://severalnines.com/blog/mysql-docker-running-galera-cluster-kubernetes
** TODO Try Functions as a Service - a serverless framework for Docker & Kubernetes http://docs.get-faas.com/
https://blog.alexellis.io/first-faas-python-function/
** TODO [#A] k8s clustering elasticsearch
https://blog.alexellis.io/kubernetes-kubeadm-video/
** TODO k8s scale with redis
** TODO k8s scale with mysqld
** TODO [#A] k8s: https://5pi.de/2016/11/20/15-producation-grade-kubernetes-cluster/
** TODO Try kops with k8s
** TODO k8s free course: https://classroom.udacity.com/courses/ud615
** TODO feedbackup for k8s study project
Aaron Mulholland [1:18 AM]
So it looks pretty good. Got some good concepts in early on. Couple of suggestions for further work;

Potentially the following scenarios;
    * Setting up ingresses and TLS
              * Fully configure something like Nginx Ingress Controller or Traefik.
              * Create TLS Secrets within Kubernetes, and use them in your ingress controller.
    * Managing RBAC  (Don't know enough about this one, but sounds like a good concept to include)
              * Creating new roles, etc

I'll have a think and if anymore come to me, I'll let you know.


Denny Zhang (Github . Blogger)
[1:19 AM]
:thumbsup:

Will update per your suggestions tomorrow, Aaron
** TODO k8s add DNS chanllenges
Gui [4:01 PM]
Getting familiar with the concepts like pod, service, RC, deployment, etc.


[4:02]
Try volume


[4:02]
DNS.


Denny Zhang (Github . Blogger)
[4:02 PM]
I'm trying to cover the volume via mysql scenarios


Gui [4:02 PM]
And other addons
1 reply Today at 4:03 PM View thread


Denny Zhang (Github . Blogger)
[4:02 PM]
For DNS, not sure whether I get your point


Gui [4:03 PM]
I haven't tried a lot myself.
1 reply Today at 4:03 PM View thread


[4:03]
Like every pod and service has an DNS name to talk to each other.


Denny Zhang (Github . Blogger) [4:04 PM]
Yes, that makes sense


[4:04]
For addons, do you have any recommended scenario?
** TODO k8s add challenge of addon
https://www.cncf.io

https://kubernetes.io/docs/concepts/cluster-administration/addons/
** TODO k8s networking models
** TODO k8s example: https://github.com/kubernetes/examples
** TODO Blog: Wordpress powered by k8s, docker swarm
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** TODO [#A] absord: https://github.com/kubecamp/kubernetes_in_one_day
** TODO [#A] absord: https://github.com/kubecamp/kubernetes_in_2_days
** DONE kubectl config view
   CLOSED: [2017-12-31 Sun 10:40]
** DONE [#A] kubernetes persistent volume claim pending
  CLOSED: [2017-12-31 Sun 11:32]
https://github.com/openshift/origin/issues/7170

kubectl get pvc
kubectl get pv

#+BEGIN_EXAMPLE
ubuntu@k8s1:~$ kubectl describe pvc
Name:          ironic-gerbil-jenkins
Namespace:     default
StorageClass:
Status:        Pending
Volume:
Labels:        app=ironic-gerbil-jenkins
               chart=jenkins-0.10.2
               heritage=Tiller
               release=ironic-gerbil
Annotations:   <none>
Capacity:
Access Modes:
Events:
  Type    Reason         Age                 From                         Message
  ----    ------         ----                ----                         -------
  Normal  FailedBinding  37s (x261 over 2h)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set


Name:          my-mysql-mysql
Namespace:     default
StorageClass:
Status:        Pending
Volume:
Labels:        app=my-mysql-mysql
               chart=mysql-0.3.2
               heritage=Tiller
               release=my-mysql
Annotations:   <none>
Capacity:
Access Modes:
Events:
  Type    Reason         Age              From                         Message
  ----    ------         ----             ----                         -------
  Normal  FailedBinding  7s (x5 over 1m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set
#+END_EXAMPLE
** DONE kubernetes start a container for testing: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il
   CLOSED: [2017-12-31 Sun 11:26]
** DONE [#A] ReplicaSet is the next-generation Replication Controller.
  CLOSED: [2017-12-04 Mon 11:26]
The only difference between a ReplicaSet and a Replication Controller right now is the selector support.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/

https://github.com/arun-gupta/oreilly-kubernetes-book/blob/master/ch01/wildfly-replicaset.yml
Next generation Replication Controller

Set-based selector requirement
- Expression: key, operator, value
- Operators: In, NotIn, Exists, DoesNotExist

▪Generally created with Deployment
▪Enables Horizontal Pod Autoscaling
** DONE k8s yaml API version: https://kubernetes.io/docs/reference/federation/extensions/v1beta1/definitions/
   CLOSED: [2017-12-03 Sun 12:50]
** DONE k8s cronjob
  CLOSED: [2018-01-03 Wed 12:26]
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/

kubectl create -f ./cronjob.yaml
kubectl get cronjob hello
kubectl get jobs --watch
kubectl delete cronjob hello

#+BEGIN_EXAMPLE
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
#+END_EXAMPLE
** DONE [#B] check k8s status: kubectl get cs
   CLOSED: [2018-01-03 Wed 11:57]
** BYPASS crictl not found in system path: warning
   CLOSED: [2018-01-03 Wed 12:36]
** DONE kubernetes default service type: ClusterIP
   CLOSED: [2018-01-02 Tue 11:07]
** DONE kubectl get nodes: Unable to connect to the server: x509: certificate signed by unknown authority: incorrect /etc/kubernetes/admin.conf
  CLOSED: [2018-01-04 Thu 00:09]


root@k8s1:~# kubectl get nodes
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
root@k8s1:~# echo $KUBECONFIG

root@k8s1:~# export KUBECONFIG=/etc/kubernetes/admin.conf
root@k8s1:~# kubectl get nodes
NAME      STATUS     ROLES     AGE       VERSION
k8s1      Ready      master    29m       v1.9.0
k8s2      NotReady   <none>    17m       v1.9.0
** DONE [#A] kubernetes-the-hard-way: https://github.com/kelseyhightower/kubernetes-the-hard-way
   CLOSED: [2017-12-04 Mon 15:49]
*** CANCELED k8s hardway: etcdctl: Error:  context deadline exceeded
  CLOSED: [2017-12-04 Mon 17:54]
https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/e8d728d0162ebcdf951464caa8be3a5b156eb463/docs/07-bootstrapping-etcd.md
#+BEGIN_EXAMPLE
mac@controller-0:~$ ETCDCTL_API=3 etcdctl member list
Error:  context deadline exceeded
#+END_EXAMPLE

#+BEGIN_EXAMPLE
mac@controller-0:~$ kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                          ERROR
etcd-2               Unhealthy   Get https://10.240.0.12:2379/health: dial tcp 10.240.0.12:2379: getsockopt: connection refused
controller-manager   Healthy     ok
etcd-1               Unhealthy   Get https://10.240.0.11:2379/health: dial tcp 10.240.0.11:2379: getsockopt: connection refused
scheduler            Healthy     ok
etcd-0               Unhealthy   Get https://10.240.0.10:2379/health: net/http: TLS handshake timeout
#+END_EXAMPLE
** DONE k8s livenessProbe(when to restart a Container), readinessProbe(when is ready to accept requests)
  CLOSED: [2018-01-08 Mon 07:41]
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
http://kubernetesbyexample.com/healthz/
https://kubernetes-v1-4.github.io/docs/user-guide/liveness/
https://github.com/arun-gupta/kubernetes-java-sample/blob/master/wildfly-pod-hc-http.yaml
http://kubernetesbyexample.com/healthz/

Probes have a number of fields that you can use to more precisely control the behavior of liveness and readiness checks:

initialDelaySeconds: Number of seconds after the container has started before liveness or readiness probes are initiated.
periodSeconds: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.
timeoutSeconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.
successThreshold: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1.
failureThreshold: When a Pod starts and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of liveness probe means restarting the Pod. In case of readiness probe the Pod will be marked Unready. Defaults to 3. Minimum value is 1.

#+BEGIN_EXAMPLE
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - args:
    - /bin/sh
    - -c
    - echo ok > /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600
    image: gcr.io/google_containers/busybox
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/health
      initialDelaySeconds: 15
      timeoutSeconds: 1
    name: liveness
#+END_EXAMPLE
** CANCELED k8s exec try to find bash, but I only have sh
   CLOSED: [2018-01-08 Mon 08:23]
** DONE list all critical pods
  CLOSED: [2018-01-04 Thu 10:10]
kubectl --namespace kube-system get pods

for pod in $(kubectl --namespace kube-system get pods -o jsonpath="{.items[*].metadata.name}"); do
    node_info=$(kubectl --namespace kube-system describe pod $pod | grep "Node:")
    echo "Pod: $pod, $node_info"
done
** DONE k8s cheatsheet: kube-shell https://github.com/cloudnativelabs/kube-shell
   CLOSED: [2017-12-31 Sun 10:47]
** DONE k8s configmap
  CLOSED: [2018-01-08 Mon 10:32]
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
| Name                                                | Summary |
|-----------------------------------------------------+---------|
| kubectl get configmaps my-wordpress-mariadb -o yaml |         |
** DONE [#A] k8s initContainers debug: kubectl logs <pod-name> -c <init-container-2>
  CLOSED: [2018-01-05 Fri 16:29]
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/
** DONE Use GCE to setup k8s cluster deployment
  CLOSED: [2018-01-07 Sun 07:26]
https://github.com/kelseyhightower/kubernetes-the-hard-way

https://cloud.google.com/
source /Users/mac/Downloads/google-cloud-sdk/completion.bash.inc
source /Users/mac/Downloads/google-cloud-sdk/path.bash.inc
*** doc: gcloud setup
#+BEGIN_EXAMPLE
   [28] us-central1-f
   [29] us-central1-c
   [30] us-central1-b
   [31] us-east1-d
   [32] us-east1-c
   [33] us-east1-b
   [34] us-east4-c
   [35] us-east4-a
   [36] us-east4-b
   [37] us-west1-a
   [38] us-west1-c
   [39] us-west1-b
   [40] Do not set default zone
  Please enter numeric choice or text value (must exactly match list
  item):  36

  Your project default Compute Engine zone has been set to [us-east4-b].
  You can change it by running [gcloud config set compute/zone NAME].

  Your project default Compute Engine region has been set to [us-east4].
  You can change it by running [gcloud config set compute/region NAME].

  Created a default .boto configuration file at [/Users/mac/.boto]. See this file and
  [https://cloud.google.com/storage/docs/gsutil/commands/config] for more
  information about configuring Google Cloud Storage.
  Your Google Cloud SDK is configured and ready to use!

  * Commands that require authentication will use denny.zhang001@gmail.com by default
  * Commands will reference project `denny-k8s-test1` by default
  * Compute Engine commands will use region `us-east4` by default
  * Compute Engine commands will use zone `us-east4-b` by default

  Run `gcloud help config` to learn how to change individual settings

  This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.
  Run `gcloud topic configurations` to learn more.

  Some things to try next:

  * Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.
  * Run `gcloud topic -h` to learn about advanced features of the SDK like arg files and output formatting
#+END_EXAMPLE
*** TODO [#A] can't find gcloud                                   :IMPORTANT:
source /Users/mac/Downloads/google-cloud-sdk/completion.bash.inc
source /Users/mac/Downloads/google-cloud-sdk/path.bash.inc
** DONE kubectl get pod
   CLOSED: [2018-04-28 Sat 09:28]
 /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

 #+BEGIN_EXAMPLE
 Your Kubernetes master has initialized successfully!

 To start using your cluster, you need to run the following as a regular user:

   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config

 You should now deploy a pod network to the cluster.
 Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
   https://kubernetes.io/docs/concepts/cluster-administration/addons/
 #+END_EXAMPLE
** DONE pod CrashLoopBackOff: starting, then crashing, then starting again and crashing again.

   CLOSED: [2018-01-05 Fri 15:47]
 https://www.krenger.ch/blog/crashloopbackoff-and-how-to-fix-it/

 https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/

| Status                     | Meaning                                                     |
|----------------------------+-------------------------------------------------------------|
| Init:N/M                   | The Pod has M Init Containers, and N have completed so far. |
| Init:Error                 | An Init Container has failed to execute.                    |
| Init:CrashLoopBackOff      | An Init Container has failed repeatedly.                    |
| Pending                    | The Pod has not yet begun executing Init Containers.        |
| PodInitializing or Running | The Pod has already finished executing Init Containers.     |
** DONE k8s ImagePullBackOff: describe pod $pod_name; No space
   CLOSED: [2018-06-25 Mon 14:28]
** DONE default pods for single node installation
   CLOSED: [2018-04-28 Sat 08:49]
 #+BEGIN_EXAMPLE
 root@mdm-k8s-node2:~# docker ps
 CONTAINER ID        IMAGE                                                                                                              COMMAND                  CREATED             STATUS              PORTS               NAMES
 75d08dd2b171        k8s.gcr.io/kube-proxy-amd64@sha256:c7036a8796fd20c16cb3b1cef803a8e980598bff499084c29f3c759bdb429cd2                "/usr/local/bin/ku..."   16 hours ago        Up 16 hours                             k8s_kube-proxy_kube-proxy-jmcs9_kube-system_02a0eac8-4a75-11e8-afce-7aa5a78d07bd_0
 0a769558ec4f        k8s.gcr.io/pause-amd64:3.1                                                                                         "/pause"                 16 hours ago        Up 16 hours                             k8s_POD_kube-proxy-jmcs9_kube-system_02a0eac8-4a75-11e8-afce-7aa5a78d07bd_0
 2af1fbfd581a        k8s.gcr.io/kube-apiserver-amd64@sha256:1ba863c8e9b9edc6d1329ebf966e4aa308ca31b42a937b4430caf65aa11bdd12            "kube-apiserver --..."   16 hours ago        Up 16 hours                             k8s_kube-apiserver_kube-apiserver-mdm-k8s-node2_kube-system_fee65b809c1e455cf1672ebe7efc4bc7_0
 63c214ac8d1b        k8s.gcr.io/kube-controller-manager-amd64@sha256:922ac89166ea228cdeff43e4c445a5dc4204972cc0e265a8762beec07b6238bf   "kube-controller-m..."   16 hours ago        Up 16 hours                             k8s_kube-controller-manager_kube-controller-manager-mdm-k8s-node2_kube-system_5ad7a10c5a8589117db7258c7d499a33_0
 324ff1a8d357        k8s.gcr.io/kube-scheduler-amd64@sha256:5f50a339f66037f44223e2b4607a24888177da6203a7bc6c8554e0f09bd2b644            "kube-scheduler --..."   16 hours ago        Up 16 hours                             k8s_kube-scheduler_kube-scheduler-mdm-k8s-node2_kube-system_aa8d5cab3ea096315de0c2003230d4f9_0
 dce77d944669        k8s.gcr.io/etcd-amd64@sha256:68235934469f3bc58917bcf7018bf0d3b72129e6303b0bef28186d96b2259317                      "etcd --listen-cli..."   16 hours ago        Up 16 hours                             k8s_etcd_etcd-mdm-k8s-node2_kube-system_59f847fe34319ab1263f0b3ee03df8a3_0
 2af621e52e11        k8s.gcr.io/pause-amd64:3.1                                                                                         "/pause"                 16 hours ago        Up 16 hours                             k8s_POD_kube-apiserver-mdm-k8s-node2_kube-system_fee65b809c1e455cf1672ebe7efc4bc7_0
 bdc64588b27d        k8s.gcr.io/pause-amd64:3.1                                                                                         "/pause"                 16 hours ago        Up 16 hours                             k8s_POD_kube-controller-manager-mdm-k8s-node2_kube-system_5ad7a10c5a8589117db7258c7d499a33_0
 14dd26427abf        k8s.gcr.io/pause-amd64:3.1                                                                                         "/pause"                 16 hours ago        Up 16 hours                             k8s_POD_kube-scheduler-mdm-k8s-node2_kube-system_aa8d5cab3ea096315de0c2003230d4f9_0
 17bfbb8af205        k8s.gcr.io/pause-amd64:3.1                                                                                         "/pause"                 16 hours ago        Up 16 hours                             k8s_POD_etcd-mdm-k8s-node2_kube-system_59f847fe34319ab1263f0b3ee03df8a3_0
 #+END_EXAMPLE
** DONE One pod may have multiple containers
   CLOSED: [2018-06-19 Tue 14:31]
 If a pod has more than 1 containers then you need to provide the name of the specific container.
** DONE kubectl edit deployment parameters
   CLOSED: [2018-04-15 Sun 21:49]
 https://github.com/kubernetes/helm/issues/2464
 kubectl -n kube-system patch deployment tiller-deploy -p '{"spec": {"template": {"spec": {"automountServiceAccountToken": true}}}}'

 kubectl --namespace=kube-system edit deployment/tiller-deploy and changed automountServiceAccountToken to true.
** DONE [#A] k8s sidecar
   CLOSED: [2018-07-15 Sun 22:50]
 https://k8s.io/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml
 #+BEGIN_EXAMPLE
 apiVersion: v1
 kind: Pod
 metadata:
   name: counter
 spec:
   containers:
   - name: count
     image: busybox
     args:
     - /bin/sh
     - -c
     - >
       i=0;
       while true;
       do
         echo "$i: $(date)" >> /var/log/1.log;
         echo "$(date) INFO $i" >> /var/log/2.log;
         i=$((i+1));
         sleep 1;
       done
     volumeMounts:
     - name: varlog
       mountPath: /var/log
   - name: count-log-1
     image: busybox
     args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
     volumeMounts:
     - name: varlog
       mountPath: /var/log
   - name: count-log-2
     image: busybox
     args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
     volumeMounts:
     - name: varlog
       mountPath: /var/log
   volumes:
   - name: varlog
     emptyDir: {}
 #+END_EXAMPLE
** TODO [#A] k8s debug why termination takes time
** TODO Kubernets availablity
*** TODO Building High-Availability Clusters: https://kubernetes.io/docs/admin/high-availability/
** TODO [#A] Blog: Kubernetes Service Type: NodePort, ClusterIP and Loadbalancer?
#+BEGIN_EXAMPLE
https://kubernetes.io/docs/concepts/services-networking/service/

Publishing services - service types
For some parts of your application (e.g. frontends) you may want to expose a Service onto an external (outside of your cluster) IP address.

Kubernetes ServiceTypes allow you to specify what kind of service you want. The default is ClusterIP.

Type values and their behaviors are:

ClusterIP: Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. This is the default ServiceType.
NodePort: Exposes the service on each Node's IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You'll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
LoadBalancer: Exposes the service externally using a cloud provider's load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created.
ExternalName: Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. This requires version 1.7 or higher of kube-dns.
#+END_EXAMPLE
*** Type: Loadbalancer
*** Type: ClusterIP
*** Type: NodePort
If you set the type field to "NodePort", the Kubernetes master will allocate a port from a flag-configured range (default: 30000-32767)
*** #  --8<-------------------------- separator ------------------------>8-- :noexport:
*** TODO Now if i access IP:NodePort, will it balance the load across multiple pods ?
https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/
#+BEGIN_EXAMPLE
Vivek Yadav [8:34 AM]
Hey Denny, quick question -

```
---
 apiVersion: v1
 kind: Service
 metadata:
   name: span
   labels:
     app: span
 spec:
   type: NodePort
   ports:
     - port: 80
       nodePort: 30080
   selector:
     app: spa

---
 apiVersion: apps/v1beta2
 kind: Deployment
 metadata:
   name: spa
 spec:
   replicas: 2
   selector:
     matchLabels:
       app: spa
   template:
     metadata:
       labels:
         app: spa
     spec:
       containers:
         - name: py
           image: viveky4d4v/local-simple-python:latest
           ports:
             - containerPort: 8080
         - name: nginx
           image: viveky4d4v/local-nginx-lb:latest
           ports:
             - containerPort: 80
       imagePullSecrets:
         - name: regsecret

```


Now if i access IP:NodePort, will it balance the load across multiple pods ?


Denny Zhang (Github . Blogger) [8:35 AM]
I don't think so
#+END_EXAMPLE
*** TODO How Does NodePort work behind the scene?
*** #  --8<-------------------------- separator ------------------------>8-- :noexport:
*** TODO How Loadbalancer is implemented in code?
*** #  --8<-------------------------- separator ------------------------>8-- :noexport:
*** TODO Does Loadbalancer works only for public cloud?
*** TODO How I configure Ingress?
** TODO [#A] NodePort VS clusterIP                                 :IMPORTANT:
https://stackoverflow.com/questions/41509439/whats-the-difference-between-clusterip-nodeport-and-loadbalancer-service-types
http://weezer.su/kubernetes-1.html
https://docs.openshift.com/container-platform/3.3/dev_guide/getting_traffic_into_cluster.html

clusterIP: You can only access this service while inside the cluster.
** TODO [#A] k8s feature watch list
*** I want to check pod initContainer logs, but I don't want to specify initContainer by name
#+BEGIN_EXAMPLE
macs-MacBook-Pro:Scenario-401 mac$ kubectl logs my-jenkins-jenkins-89889ddb7-ct7jw -c 1
Error from server (BadRequest): container 1 is not valid for pod my-jenkins-jenkins-89889ddb7-ct7jw
macs-MacBook-Pro:Scenario-401 mac$ kubectl logs my-jenkins-jenkins-89889ddb7-ct7jw -c  copy-default-config
Error from server (BadRequest): container "copy-default-config" in pod "my-jenkins-jenkins-89889ddb7-ct7jw" is waiting to start: PodInitializing
macs-MacBook-Pro:Scenario-401 mac$ kubectl logs my-jenkins-jenkins-89889ddb7-ct7jw -c  copy-default-config
Error from server (BadRequest): container "copy-default-config" in pod "my-jenkins-jenkins-89889ddb7-ct7jw" is waiting to start: PodInitializing
#+END_EXAMPLE
*** Support using environment variables inside deployment yaml file
https://github.com/kubernetes/kubernetes/issues/52787
** TODO pod error: CreateContainerConfigError
https://github.com/kubernetes/minikube/issues/2256
#+BEGIN_EXAMPLE
bash-3.2$ kubectl get pod my-wordpress-wordpress-df987548d-btvf5
NAME                                     READY     STATUS                       RESTARTS   AGE
my-wordpress-wordpress-df987548d-btvf5   0/1       CreateContainerConfigError   0          2m
bash-3.2$
#+END_EXAMPLE

#+BEGIN_EXAMPLE
bash-3.2$ kubectl describe pod my-wordpress-wordpress-df987548d-btvf5
Name:           my-wordpress-wordpress-df987548d-btvf5
Namespace:      default
Node:           minikube/192.168.99.102
Start Time:     Fri, 05 Jan 2018 16:41:27 -0600
Labels:         app=my-wordpress-wordpress
                pod-template-hash=895431048
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"default","name":"my-wordpress-wordpress-df987548d","uid":"910e01e0-f269-11e7-b6d8...
Status:         Pending
IP:             172.17.0.6
Created By:     ReplicaSet/my-wordpress-wordpress-df987548d
Controlled By:  ReplicaSet/my-wordpress-wordpress-df987548d
Containers:
  my-wordpress-wordpress:
    Container ID:
    Image:          bitnami/wordpress:4.9.1-r1
    Image ID:
    Ports:          80/TCP, 443/TCP
    State:          Waiting
      Reason:       CreateContainerConfigError
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      300m
      memory:   512Mi
    Liveness:   http-get http://:http/wp-login.php delay=120s timeout=5s period=10s #success=1 #failure=6
    Readiness:  http-get http://:http/wp-login.php delay=30s timeout=3s period=5s #success=1 #failure=3
    Environment:
      ALLOW_EMPTY_PASSWORD:         yes
      MARIADB_ROOT_PASSWORD:        <set to the key 'mariadb-root-password' in secret 'my-wordpress-mariadb'>  Optional: false
      MARIADB_HOST:                 my-wordpress-mariadb
      MARIADB_PORT_NUMBER:          3306
      WORDPRESS_DATABASE_NAME:      bitnami_wordpress
      WORDPRESS_DATABASE_USER:      bn_wordpress
      WORDPRESS_DATABASE_PASSWORD:  <set to the key 'mariadb-password' in secret 'my-wordpress-mariadb'>  Optional: false
      WORDPRESS_USERNAME:           admin
      WORDPRESS_PASSWORD:           <set to the key 'wordpress-password' in secret 'my-wordpress-wordpress'>  Optional: false
      WORDPRESS_EMAIL:              contact@dennyzhang.com
      WORDPRESS_FIRST_NAME:         FirstName
      WORDPRESS_LAST_NAME:          LastName
      WORDPRESS_BLOG_NAME:          My DevOps Blog!
      SMTP_HOST:
      SMTP_PORT:
      SMTP_USER:
      SMTP_PASSWORD:                <set to the key 'smtp-password' in secret 'my-wordpress-wordpress'>  Optional: false
      SMTP_USERNAME:
      SMTP_PROTOCOL:
    Mounts:
      /bitnami/apache from wordpress-data (rw)
      /bitnami/php from wordpress-data (rw)
      /bitnami/wordpress from wordpress-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tc8kd (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  wordpress-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  my-wordpress-wordpress
    ReadOnly:   false
  default-token-tc8kd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-tc8kd
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type     Reason                 Age              From               Message
  ----     ------                 ----             ----               -------
  Normal   Scheduled              1m               default-scheduler  Successfully assigned my-wordpress-wordpress-df987548d-btvf5 to minikube
  Normal   SuccessfulMountVolume  1m               kubelet, minikube  MountVolume.SetUp succeeded for volume "pvc-910644d3-f269-11e7-b6d8-08002782d6cd"
  Normal   SuccessfulMountVolume  1m               kubelet, minikube  MountVolume.SetUp succeeded for volume "default-token-tc8kd"
  Normal   Pulled                 1s (x7 over 1m)  kubelet, minikube  Container image "bitnami/wordpress:4.9.1-r1" already present on machine
  Warning  Failed                 1s (x7 over 1m)  kubelet, minikube  Error: lstat /tmp/hostpath-provisioner/pvc-910644d3-f269-11e7-b6d8-08002782d6cd: no such file or directory
  Warning  FailedSync             1s (x7 over 1m)  kubelet, minikube  Error syncing pod
bash-3.2$
#+END_EXAMPLE
** TODO [#A] Certified Kubernetes Administrator (CKA)              :IMPORTANT:
https://www.cncf.io/certification/expert/

https://github.com/cncf/curriculum/blob/master/certified_kubernetes_administrator_exam_v1.8.0.pdf

It is an online, proctored, performance-based test that requires solving multiple issues from a command line.

Candidates have 3 hours to complete the tasks.
** HALF Difference in between selectors and labels
** TODO [#A] kubernetes mount a file to pod                        :IMPORTANT:
https://stackoverflow.com/questions/33415913/whats-the-best-way-to-share-mount-one-file-into-a-pod
https://www.linkedin.com/feed/update/urn:li:activity:6355445509146107904/
** TODO K8S label & Selector
https://github.com/dennyzhang/dennytest/tree/master/cheatsheet-kubernetes-A4][challenges-leetcode-interesting]]
* [#A] minikube                                                    :noexport:
https://github.com/kubernetes/minikube
https://github.com/dennyzhang/cheatsheet-kubernetes-A4
** DONE minikube volume local drive
   CLOSED: [2018-07-15 Sun 22:46]
 https://stackoverflow.com/questions/42456159/minikube-volumes

 #+BEGIN_EXAMPLE
 /data
 /var/lib/localkube
 /var/lib/docker
 #+END_EXAMPLE

ls -lth /var/lib/kubelet/pods/f2f8f500-88ba-11e8-89ad-080027cbaea4/volumes/kubernetes.io~empty-dir/varlog/1.log
* [#A] kubernetes/helm: The Kubernetes Package Manager             :noexport:
| Name    | Summary                                                                  |
|---------+--------------------------------------------------------------------------|
| Helm    | a chart manager.                                                         |
| Charts  | packages of pre-configured Kubernetes resources.                         |
| Release | a collection of Kubernetes resources deployed to the cluster using Helm. |
| tiller  | helm server manages releases (installations) of your charts.             |

| Name                                           | Summary |
|------------------------------------------------+---------|
| helm init                                      |         |
| helm list                                      |         |
| helm list -a                                   |         |
| helm repo update                               |         |
| helm install stable/mysql                      |         |
| helm install --name mysql-release stable/mysql |         |
| helm inspect stable/mysql                      |         |
| helm status $helm_name                         |         |
| helm delete $helm_name                         |         |
| helm delete --purge $helm_name                 |         |
|------------------------------------------------+---------|
| ~/.helm/cache/archive                          |         |

Release, list, inspect, delete, rollback, purge
** useful link
https://github.com/kubernetes/helm
https://hub.kubeapps.com/
https://daemonza.github.io/2017/02/20/using-helm-to-deploy-to-kubernetes/
https://www.mirantis.com/blog/install-kubernetes-apps-helm/
** DONE Use helm to install mysql
   CLOSED: [2018-01-05 Fri 13:09]
https://github.com/kubernetes/charts/tree/master/cheatsheet-kubernetes-A4][challenges-leetcode-interesting]]
*** helm inspect stable/mysql
#+BEGIN_EXAMPLE
ubuntu@k8s1:~$ helm inspect stable/mysql
description: Fast, reliable, scalable, and easy to use open-source relational database
  system.
engine: gotpl
home: https://www.mysql.com/
icon: https://www.mysql.com/common/logos/logo-mysql-170x115.png
keywords:
- mysql
- database
- sql
maintainers:
- email: viglesias@google.com
  name: Vic Iglesias
name: mysql
sources:
- https://github.com/kubernetes/charts
- https://github.com/docker-library/mysql
version: 0.3.2

---
## mysql image version
## ref: https://hub.docker.com/r/library/mysql/tags/
##
image: "mysql"
imageTag: "5.7.14"

## Specify password for root user
##
## Default: random 10 character string
# mysqlRootPassword: testing

## Create a database user
##
# mysqlUser:
# mysqlPassword:

## Allow unauthenticated access, uncomment to enable
##
# mysqlAllowEmptyPassword: true

## Create a database
##
# mysqlDatabase:

## Specify an imagePullPolicy (Required)
## It's recommended to change this to 'Always' if the image tag is 'latest'
## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
##
imagePullPolicy: IfNotPresent

livenessProbe:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 3

## Persist data to a persistent volume
persistence:
  enabled: true
  ## database data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  size: 8Gi

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  requests:
    memory: 256Mi
    cpu: 100m

# Custom mysql configuration files used to override default mysql settings
configurationFiles:
#  mysql.cnf: |-
#    [mysqld]
#    skip-name-resolve
#+END_EXAMPLE
*** helm install stable/mysql
#+BEGIN_EXAMPLE
ubuntu@k8s1:~$ helm install stable/mysql
NAME:   joyous-grizzly
LAST DEPLOYED: Sun Dec 31 14:28:07 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/Secret
NAME                  TYPE    DATA  AGE
joyous-grizzly-mysql  Opaque  2     0s

==> v1/PersistentVolumeClaim
NAME                  STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE
joyous-grizzly-mysql  Pending  0s

==> v1/Service
NAME                  TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE
joyous-grizzly-mysql  ClusterIP  10.100.217.119  <none>       3306/TCP  0s

==> v1beta1/Deployment
NAME                  DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
joyous-grizzly-mysql  1        1        1           0          0s

==> v1/Pod(related)
NAME                                  READY  STATUS   RESTARTS  AGE
joyous-grizzly-mysql-8bb45c5bf-b4kqv  0/1    Pending  0         0s


NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
joyous-grizzly-mysql.default.svc.cluster.local

To get your root password run:

    kubectl get secret --namespace default joyous-grizzly-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h joyous-grizzly-mysql -p
#+END_EXAMPLE
*** DONE no available release name found
   CLOSED: [2017-12-31 Sun 08:50]
https://github.com/kubernetes/helm/issues/3055
** BYPASS [#A] helm install Error: no available release name found
   CLOSED: [2018-01-05 Fri 13:09]
https://github.com/kubernetes/helm/issues/3055
https://stackoverflow.com/questions/43499971/helm-error-no-available-release-name-found
https://github.com/kubernetes/helm/issues/2224

disable RBAC, then it works

kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts

#+BEGIN_EXAMPLE
ubuntu@k8s1:~$ helm install stable/mysql
Error: no available release name found
#+END_EXAMPLE
** DONE Create persisitvolume first, before trying helm
   CLOSED: [2017-12-31 Sun 11:14]
sudo mkdir -p /data
sudo chmod 777 /data

cat > pv.yaml <<EOF
kind: PersistentVolume
apiVersion: v1
metadata:
  name: mydata
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/data/mydata"
EOF

kubectl apply -f ./pv.yaml
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** HALF ubuntu install helm
https://github.com/kubernetes/helm/releases/tag/v2.9.1
https://docs.helm.sh/using_helm/#quickstart-guide

- Download binary
- helm init
** HALF helm list: Error: could not find a ready tiller pod: kubectl get pods --all-namespaces
#+BEGIN_EXAMPLE
denny@mdm-k8s-node2:/root$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
kube-system   etcd-mdm-k8s-node2                      1/1       Running   0          3m
kube-system   kube-apiserver-mdm-k8s-node2            1/1       Running   0          3m
kube-system   kube-controller-manager-mdm-k8s-node2   1/1       Running   0          3m
kube-system   kube-dns-86f4d74b45-rf5rj               0/3       Pending   0          4m
kube-system   kube-proxy-czcs5                        1/1       Running   0          4m
kube-system   kube-scheduler-mdm-k8s-node2            1/1       Running   0          3m
kube-system   tiller-deploy-df4fdf55d-zxdq4           0/1       Pending   0          1m
#+END_EXAMPLE
** DONE helm: Error: could not find a ready tiller pod
   CLOSED: [2018-04-15 Sun 21:50]
 https://github.com/kubernetes/helm/issues/2064
 kubectl -n kube-system get po

 kubectl --namespace kube-system describe pod tiller-deploy-6d5c5f8457-6w22m
** DONE helm list error
   CLOSED: [2018-04-15 Sun 21:50]
 https://github.com/kubernetes/helm/issues/2464

 I meet the same problem, it was due to KUBECONFIG is not default. so you shoud specify KUBECONFIG env to the right position.

 Denny-Laptop:~ DennyZhang$ helm list
 Error: Get http://localhost:8080/api/v1/namespaces/kube-system/configmaps?labelSelector=OWNER%!D(MISSING)TILLER: dial tcp 127.0.0.1:8080: connect: connection refused

 #+BEGIN_EXAMPLE
 @mattus Thanks a lot, i was stuck for ~ 3 days with this at work trying to deploy a k8s cluster. This should really be documented somewhere.
 What i did to solve the issue was:

 kubectl --namespace=kube-system edit deployment/tiller-deploy and changed automountServiceAccountToken to true.
 Then 'helm list' was giving me:
 Error: configmaps is forbidden: User "system:serviceaccount:kube-system:default" cannot list configmaps in the namespace "kube-system"
 That was fixed with solution from #2687:
 kubectl --namespace=kube-system create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
 #+END_EXAMPLE
** DONE helm start with env configured: helm install --set Master.ServiceType=NodePort stable/jenkins
   CLOSED: [2018-04-15 Sun 21:50]
<<<<<<< HEAD
* [#A] CRD: challenges-k8s-crd                                     :noexport:
<<<<<<< HEAD
* [#A] k8s metric server                                 :noexport:IMPORTANT:
Metrics Server is a cluster-wide aggregator of resource usage data. 

Metrics Server registered in the main API server through Kubernetes aggregator.

https://github.com/kubernetes-incubator/metrics-server
https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy/1.8%2B

https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/
| Name           | Summary                                                           |
|----------------+-------------------------------------------------------------------|
| Core metrics   | node/container level metrics; CPU, memory, disk and network, etc. |
| Custom metrics | refers to application metrics, e.g. HTTP request rate.            |

Today (Kubernetes 1.7), there are several sources of metrics within a Kubernetes cluster
| Name           | Summary                                                             |
|----------------+---------------------------------------------------------------------|
| Heapster       | k8s add-on                                                          |
| Cadvisor       | a standalone container/node metrics collection and monitoring tool. |
| Kubernetes API | does not track metrics. But can get real time metrics               |

** metric server
Resource Metrics API is an effort to provide a first-class Kubernetes API (stable, versioned, discoverable, available through apiserver and with client support) that serves resource usage metrics for pods and nodes.

- metric server is sort of a stripped-down version of Heapster
- The metrics-server will collect "Core" metrics from cAdvisor APIs (currently embedded in the kubelet) and store them in memory as opposed to in etcd. 
- The metrics-server will provide a supported API for feeding schedulers and horizontal pod auto-scalers
- All other Kubernetes components will supply their own metrics in a Prometheus format
** Cadvisor
Cadvisor monitors node and container core metrics in addition to container events.
It natively provides a Prometheus metrics endpoint
The Kubernetes kublet has an embedded Cadvisor that only exposes the metrics, not the events.
** heapster
Heapster is an add on to Kubernetes that collects and forwards both node, namespace, pod and container level metrics to one or more "sinks" (e.g. InfluxDB). 

It also provides REST endpoints to gather those metrics. The metrics are constrained to CPU, filesystem, memory, network and uptime.

Heapster queries the kubelet for its data.

Today, heapster is the source of the time-series data for the Kubernetes Dashboard.
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** TODO How to query metric server
** TODO Key scenarios of metric server
The metrics-server will provide a much needed official API for the internal components of Kubernetes to make decisions about the utilization and performance of the cluster.

- HPA(Horizontal Pod Autoscaler) need input to do good auto-scaling
** TODO There are plans for an "Infrastore", a Kubernetes component that keeps historical data and events
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** TODO why from heapster to k8s metric server?
** TODO kube-aggregator
** TODO what is promethues format?
#+BEGIN_EXAMPLE
Denny Zhang [12:34 AM]
An easy introduction about k8s metric server. (It will replace heapster)

https://blog.freshtracks.io/what-is-the-the-new-kubernetes-metrics-server-849c16aa01f4

> All other Kubernetes components will supply their own metrics in a Prometheus format

In logging domain, we can say `syslog` is the standard format

In metric domain, maybe we can choose `prometheus` as the standard format.
#+END_EXAMPLE
** try metric server in minikube
https://docs.giantswarm.io/guides/kubernetes-heapster/

http://192.168.99.102:30000/metrics
** Metrics Use Cases
https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md

https://docs.giantswarm.io/guides/kubernetes-heapster/

#+BEGIN_EXAMPLE
Horizontal Pod Autoscaler: It scales pods automatically based on CPU or custom metrics (not explained here). More information here.
Kubectl top: The command top of our beloved Kubernetes CLI display metrics directly in the terminal.
Kubernetes dashboard: See Pod and Nodes metrics integrated into the main Kubernetes UI dashboard. More info here
Scheduler: In the future Core Metrics will be considered in order to schedule best-effort Pods.
#+END_EXAMPLE
** useful link
https://blog.freshtracks.io/what-is-the-the-new-kubernetes-metrics-server-849c16aa01f4
https://blog.outlyer.com/monitoring-kubernetes-with-heapster-and-prometheus
https://www.outcoldman.com/en/archive/2017/07/09/kubernetes-monitoring-resources/
<<<<<<< HEAD
* k8s loadbalancer                                                 :noexport:
** DONE k8s service: loadbalancer
   CLOSED: [2018-06-19 Tue 13:51]
 #+BEGIN_EXAMPLE
 cat > service.yml <<EOF
 apiVersion: v1
 kind: Service
 metadata:
   name: lb
   namespace: logging
 spec:
   selector:
     app: kibana
   ports:
   - protocol: TCP
     port: 5601
   type: LoadBalancer
 EOF
 #+END_EXAMPLE
* k8s DaemonSet                                                    :noexport:
** DONE k8s daemonsets: ensures that all (or some) Nodes run a copy of a Pod.
   CLOSED: [2018-06-19 Tue 13:28]
 https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/

 As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

 Some typical uses of a DaemonSet are:

 - running a cluster storage daemon, such as glusterd, ceph, on each node.
 - running a logs collection daemon on every node, such as fluentd or logstash.
   - running a node monitoring daemon on every node, such as Prometheus Node Exporter, collectd, Datadog agent, New Relic agent, or Ganglia gmond.
* [#A] etcd                                                        :noexport:
https://coreos.com/etcd/docs/latest/dev-guide/interacting_v3.html
https://coreos.com/etcd/docs/latest/v2/README.html
* [#B] k8s addons                                                  :noexport:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
** DONE k8s install add-on: dashboard
  CLOSED: [2018-01-03 Wed 12:19]
- Install, then use kubectl-proxy to start
- Create user and binding, then use token to login

#+BEGIN_EXAMPLE
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
nohup kubectl proxy --port=8001 --address=0.0.0.0 &

curl http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

#+END_EXAMPLE

#+BEGIN_EXAMPLE
# https://github.com/kubernetes/dashboard/wiki/Creating-sample-user
cat > user.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF
#+END_EXAMPLE

kubectl apply -f user.yaml
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

https://github.com/kubernetes/dashboard#kubernetes-dashboard
https://blog.frognew.com/2017/09/kubeadm-install-kubernetes-1.8.html#8dashboard%E6%8F%92%E4%BB%B6%E9%83%A8%E7%BD%B2
*** DONE kubectl proxy listen on all network nics
  CLOSED: [2018-01-03 Wed 12:12]
https://github.com/kubernetes/kubectl/issues/142
kubectl proxy --port=8001 --address=0.0.0.0
* [#A] k8s volumes                                                 :noexport:
  CLOSED: [2017-12-01 Fri 22:45]
https://kubernetes.io/docs/concepts/storage/volumes
https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes

https://blog.couchbase.com/stateful-containers-kubernetes-amazon-ebs/
https://stackoverflow.com/questions/37555281/create-kubernetes-pod-with-volume-using-kubectl-run
https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/

▪Directory accessible to the containers in a pod
▪Volume outlives any containers in a pod
▪Common types
   hostPath
   nfs
   awsElasticBlockStore
   gcePersistentDisk

#+BEGIN_EXAMPLE
Creating and using a persistent volume is a three step process:
1. Provision: Administrator provision a networked storage in the cluster, such as AWS ElasticBlockStore volumes. This is called as PersistentVolume.
2. Request storage: User requests storage for pods by using claims. Claims can specify levels of resources (CPU and memory), specific sizes and access modes (e.g. can be mounted once read/write or many times write only).
This is called as PersistentVolumeClaim.
1. Use claim: Claims are mounted as volumes and used in pods for storage.
#+END_EXAMPLE
** DONE persistence.accessMode ReadWriteOnce or ReadOnly: https://github.com/kubernetes/charts/tree/master/cheatsheet-kubernetes-A4][challenges-leetcode-interesting]]
  CLOSED: [2018-01-02 Tue 16:52]
The access modes are:

ReadWriteOnce - the volume can be mounted as read-write by a single node
ReadOnlyMany - the volume can be mounted read-only by many nodes
ReadWriteMany - the volume can be mounted as read-write by many nodes
* [#A] kubeadm: performs the actions necessary to get a minimum viable cluster up and running :noexport:
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

kubeadm init and kubeadm join together provides a nice user experience
for creating a best-practice but bare Kubernetes cluster from scratch.

| Name            | Summary                                                                                                      |
|-----------------+--------------------------------------------------------------------------------------------------------------|
| kubeadm init    | bootstrap a Kubernetes master node                                                                           |
| kubeadm join    | bootstrap a Kubernetes worker node and join it to the cluster                                                |
| kubeadm upgrade | upgrade a Kubernetes cluster to a newer version                                                              |
| kubeadm config  | if you initialized your cluster using kubeadm v1.7.x or lower, to configure your cluster for kubeadm upgrade |
| kubeadm token   | manage tokens for kubeadm join                                                                               |
| kubeadm reset   | revert any changes made to this host by kubeadm init or kubeadm join                                         |

This process works with local VMs, physical servers and/or cloud servers.
** DONE kubeadm: recreate a node
  CLOSED: [2018-01-03 Wed 16:18]
https://stackoverflow.com/questions/45913034/readd-a-deleted-node-to-kubernetes

https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
#+BEGIN_EXAMPLE
kubeadm: run kubeadm reset and kubeadm join ... again on the node (you might need to create a new token if the original one was short-lived, see the linked doc)
most clouds: delete the VM. It will be recreated and will rejoin the cluster
others: see self registration and manual registration for details.
#+END_EXAMPLE
** TODO kubeadm join token issue: How to caculate --discovery-token-ca-cert-hash
https://stackoverflow.com/questions/47770486/kubeadm-init-token-xyz-or-kubeadm-init-token-xyz

kubeadm token  list

how to get --discovery-token-ca-cert-hash

#+BEGIN_EXAMPLE
root@k8s1:~# kubeadm init --token 2f1a31.00f66dec74fd53f3 --apiserver-advertise-address=172.42.42.1
[init] Using Kubernetes version: v1.9.0
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
	[WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k8s1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.42.42.1]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 25.502909 seconds
[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node k8s1 as master by adding a label and a taint
[markmaster] Master k8s1 tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: 2f1a31.00f66dec74fd53f3
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 2f1a31.00f66dec74fd53f3 172.42.42.1:6443 --discovery-token-ca-cert-hash sha256:26ba6d67cbffbe67a7b77748ffa1d76bab74ff546de5a9e2af930e5a1272d188
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@k8s1:/var/log# kubeadm reset
[preflight] Running pre-flight checks.
[reset] Stopping the kubelet service.
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Removing kubernetes-managed containers.
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes /var/lib/etcd]
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
#+END_EXAMPLE
** useful link
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
https://medium.com/@SystemMining/setup-kubenetes-cluster-on-ubuntu-16-04-with-kubeadm-336f4061d929
https://marc.xn--wckerlin-0za.ch/computer/kubernetes-on-ubuntu-16-04

https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.8.md
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#24-initializing-your-master
* [#B] k8s security: secrets, authentication & authorization       :noexport:
** what's service account: In contrast, service accounts are users managed by the Kubernetes API.
https://kubernetes.io/docs/admin/authentication/
https://github.com/kubernetes/kubernetes/blob/master/examples/elasticsearch/service-account.yaml
https://kubernetes.io/docs/admin/authorization/
** serviceaccount, clusterrolebinding
https://blog.frognew.com/2017/12/its-time-to-use-helm.html
#+BEGIN_EXAMPLE
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
#+END_EXAMPLE
** web page: Creating sample user · kubernetes/dashboard Wiki · GitHub
https://github.com/kubernetes/dashboard/wiki/Creating-sample-user
*** webcontent                                                     :noexport:
#+begin_example
Location: https://github.com/kubernetes/dashboard/wiki/Creating-sample-user
Skip to content
The vote is over, but the fight for net neutrality isn't. Show your support for a free and open
internet.
Learn more Dismiss

  * Features
  * Business
  * Explore
  * Marketplace
  * Pricing

This repository [                    ]
Sign in or Sign up

  * Watch 120
  * Star 1,839
  * Fork 586

kubernetes/dashboard

Code Issues 95 Pull requests 8 Projects 0 Wiki Insights

Creating sample user

Sebastian Florek edited this page Dec 14, 2017 · 6 revisions

Pages 21

[                    ]

  * Home
  * Access control
  * Accessing Dashboard
  * Accessing Dashboard 1.6.X and below
  * Accessing Dashboard 1.7.X and above
  * Architecture
  * Certificate management
  * Code conventions
  * Compatibility matrix
  * Creating sample user
  * Dashboard arguments
  * Dependency management
  * FAQ
  * Getting started
  * Installation
  * Integrations
  * Internationalization
  * Labels
  * Release procedures
  * Roadmap
  * Text conventions
  * Show 6 more pages...

 Home

 Common

  * FAQ
  * Compatibility matrix
  * Roadmap
  * Dashboard arguments

 User Guide

  * Installation
  * Certificate management
  * Accessing Dashboard
      + 1.7.X and above
      + 1.6.X and below
  * Access control
      + Creating sample user
  * Integrations
  * Labels

 Development Guide

  * Getting started
  * Release procedures
  * Dependency management
  * Architecture
  * Code conventions
  * Text conventions
  * Internationalization

Clone this wiki locally

[https://github.com/k]

In this guide, we will find out how to create a new user using Service Account mechanism of
Kubernetes, grant this user admin permissions and log in to Dashboard using bearer token tied to
this user.

Copy provided snippets to some xxx.yaml file and use kubectl create -f xxx.yaml to create them.

 Create Service Account

We are creating Service Account with name admin-user in namespace kube-system first.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

 Create ClusterRoleBinding

In most cases after provisioning our cluster using kops or kubeadm or any other popular tool admin
Role already exists in the cluster. We can use it and create only RoleBinding for our
ServiceAccount.

NOTE: apiVersion of ClusterRoleBinding resource may differ between Kubernetes versions. Starting
from v1.8 it was promoted to rbac.authorization.k8s.io/v1.

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

 Bearer Token

Now we need to find token we can use to log in. Execute following command:

kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

It should print something like:

Name:         admin-user-token-6gl6l
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=admin-user
              kubernetes.io/service-account.uid=b16afba9-dfec-11e7-bbb9-901b0e532516

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZnbDZsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiMTZhZmJhOS1kZmVjLTExZTctYmJiOS05MDFiMGU1MzI1MTYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.M70CU3lbu3PP4OjhFms8PVL5pQKj-jj4RNSLA4YmQfTXpPUuxqXjiTf094_Rzr0fgN_IVX6gC4fiNUL5ynx9KU-lkPfk0HnX8scxfJNzypL039mpGt0bbe1IXKSIRaq_9VW59Xz-yBUhycYcKPO9RM2Qa1Ax29nqNVko4vLn1_1wPqJ6XSq3GYI8anTzV8Fku4jasUwjrws6Cn6_sPEGmL54sq5R4Z5afUtv-mItTmqZZdxnkRqcJLlg2Y8WbCPogErbsaCDJoABQ7ppaqHetwfM_0yMun6ABOQbIwwl8pspJhpplKwyo700OSpvTT9zlBsu-b35lzXGBRHzv5g_RA

Now copy the token and paste it into Enter token field on log in screen. zrzut ekranu z 2017-12-14
10-58-28

Click Sign in button and that's it. You are now logged in as an admin.

zrzut ekranu z 2017-12-14 10-59-31

In order to find out more about how to grant/deny permissions in Kubernetes read official
authentication & authorization documentation.

Copyright 2017 The Kubernetes Authors

  * © 2018 GitHub, Inc.
  * Terms
  * Privacy
  * Security
  * Status
  * Help

  * Contact GitHub
  * API
  * Training
  * Shop
  * Blog
  * About

You can't perform that action at this time.
You signed in with another tab or window. Reload to refresh your session. You signed out in another
tab or window. Reload to refresh your session.

#+end_example
** k8s secrets: intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys.
https://github.com/arun-gupta/vault-kubernetes/blob/master/secrets.yaml
http://kubernetesbyexample.com/secrets/

- Secrets are namespaced objects, that is, exist in the context of a namespace
- You can access them via a volume or an environment variable from a container running in a pod
- The secret data on nodes is stored in tmpfs volumes

kubectl create secret generic mysecret --from-literal=mysql_root_password=my-secret-pw
kubectl get secret mysecret

#+BEGIN_EXAMPLE
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
  restartPolicy: Never
#+END_EXAMPLE
* HPA: Horizontal Pod Autoscaler                                   :noexport:
* Uncertainty & Uncomfortable things with K8S                      :noexport:
** Destroy namepsace takes more than 15 minutes, with nowhere to check
Testing in minikube
** Pod stucks in containercreating for a long time
* HALF kubectl apply to a list of folder: kubectl apply -R -f namespace-drain-manifests/manifests :noexport:
* GKE user access                                                  :noexport:
#+BEGIN_EXAMPLE
If y'all run into the following error: `is forbidden: attempt to grant extra privileges:` when trying to run `kubectl apply -R -f ~/workspace/namespace-drain/manifests/` against a GKE cluster, then run the following command.

```kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $(gcloud config get-value account)```
#+END_EXAMPLE
* #  --8<-------------------------- separator ------------------------>8-- :noexport:
* Blog: How Enterprise Do XXX in Container world?                  :noexport:
* TODO [#A] Blog: interview candidates for k8s experience          :noexport:
** Explain concepts
*** What's k8s context. Why we need it?
*** What's initContainer? Why we need it?
*** Network policy
** Comparision
*** configmap vs secrets
*** labels vs anonations
What are k8s Annotations? What differences it is compared with labels:

- Like labels, annotations are key/value pairs. Where labels have length limits, annotations can be quite large.
-  you can't query or select objects based on annotations.
- Are used for non-identifying information. Stuff not used internally by k8s.

https://codeengineered.com/blog/2017/kubernetes-labels-annotations/
https://vsupalov.com/kubernetes-labels-annotations-difference/ (edited)
*** clusterip, service, loadbalancer
*** ClusterRole vs Role
*** serviceaccount vs useraccount
** Scenarios/Experience
*** tell me about k8s security model
*** tell me about k8s scheduling model
*** tell me about k8s HA model
*** tell me about k8s trouble shooting experience
** Your Wish List
*** layer of yaml
*** ABBA on volumes
*** apply one configmap to all namespace
* k8s workflow: every 3 months has one new release                 :noexport:
https://github.com/kubernetes/kubeadm/blob/master/docs/release-cycle.md
* Blog: Kubernetes Limitation List                                 :noexport:
- Starting with Kubernetes 1.6 we support 5000 nodes clusters with 30 pods per node. ([[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md#scalability-limitations][link]])
* #  --8<-------------------------- separator ------------------------>8-- :noexport:
* TODO Why need kubernetes/apiserver: https://github.com/kubernetes/apiserver :noexport:
Library for writing a Kubernetes-style API server.

https://github.com/kubernetes/kube-aggregator
* TODO [#A] Questions                                              :noexport:
** pod type
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-service-is-missing-endpoints
#+BEGIN_EXAMPLE
...
spec:
  - selector:
     name: nginx
     type: frontend
#+END_EXAMPLE

kubectl get pods --selector=name=nginx,type=frontend
** Containers inside a Pod can communicate with one another using localhost. 
https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/

Networking
Each Pod is assigned a unique IP address. Every container in a Pod shares the network namespace, including the IP address and network ports. Containers inside a Pod can communicate with one another using localhost. When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports).

** How to restart a container inside a Pod?
https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/

Restarting a container in a Pod should not be confused with restarting the Pod. The Pod itself does not run, but is an environment the containers run in and persists until it is deleted.
** explain k8s components: apiserver, scheduler, controller-manager, kube-proxy
** get logs of failed container
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-is-crashing-or-otherwise-unhealthy
#+BEGIN_EXAMPLE
If your container has previously crashed, you can access the previous container's crash log with:

$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}
#+END_EXAMPLE
** Why k8s dashboard get deprecated?
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
* TODO k8s architecture                                            :noexport:
https://www.youtube.com/watch?v=_WfJz5VS_cU&list=PLj6h78yzYM2NGwRwkBPxigKio2r0XHPl9
* TODO [#A] Try minikube for latest k8s                            :noexport:
* TODO k8s scenario problems                                       :noexport:
** TODO export k8s dashboard: kube proxy VS ingress
** TODO how to back and restore etcd
https://kubernetes-incubator.github.io/kube-aws/advanced-topics/etcd-backup-and-restore.html
* Apply yamls file recursively                                     :noexport:
#+BEGIN_SRC sh
# create
time ls -1 */*.yml | grep -v namespace | xargs -I{} kubectl apply -f {}

# delete
time ls -1r */*.yml | grep -v namespace | xargs -I{} kubectl delete -f {}
#+END_SRC
* TODO problems                                                    :noexport:
** volume mount current local drive
* helm                                                             :noexport:
#+BEGIN_SRC sh
# Lets create the service account 
kubectl create serviceaccount --namespace kube-system tiller
kubectl create serviceaccount --namespace kubestack etcd-operator

# Bind the serviceaccount to the admin role called cluster-admin inside the kube-system namespace.
 kubectl create clusterrolebinding \
tiller-cluster-rule \
--clusterrole=cluster-admin \
--serviceaccount=kube-system:tiller

# Lets now initialize helm
helm init

# Update the tiller-deploy deployment to have the service account.
 kubectl patch deployment \
tiller-deploy -p \
'{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}' \
-n kube-system
#+END_SRC
